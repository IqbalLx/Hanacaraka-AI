{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IqbalLx/Hanacaraka-AI/blob/master/Hanacaraka%20AI%20-%20notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8QUgIB_SwPO"
   },
   "source": [
    "## Mount your Google Drive to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e39SifrpSuhZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "giuuon3bUMC7"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "filename = '/content/gdrive/My Drive/HanacarakaAI/datasetv3.zip' # Adjust to your own drive directory\n",
    "zip_ref = zipfile.ZipFile(filename)\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "T5UCRx7QUl96",
    "outputId": "4ed5db53-f70d-4014-dcb7-aeb6c4b02b73"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as plt_image\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "PjNuUsy4otPK",
    "outputId": "8f9d2d1f-2b6f-49d1-9ec4-d2c403e04936"
   },
   "source": [
    "# Helper function for plotting Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(hist):\n",
    "  history = hist.history\n",
    "  history['epoch'] = hist.epoch\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  plt.subplot(121)\n",
    "  plt.plot(history['epoch'], history['loss'], label='Loss')\n",
    "  plt.plot(history['epoch'], history['val_loss'], label='Val Loss', color='orange')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(122)\n",
    "  plt.plot(history['epoch'], history['accuracy'], label='Acc')\n",
    "  plt.plot(history['epoch'], history['val_accuracy'], label='Val Acc', color='orange')\n",
    "  plt.legend()\n",
    "\n",
    "  return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['ba', 'ca', 'da', 'dha', 'ga', 'ha', 'ja', 'ka', 'la', 'ma',\n",
    "           'na', 'nga', 'nya', 'pa', 'ra', 'sa', 'ta', 'tha', 'wa', 'ya']\n",
    "\n",
    "def test(model, width):\n",
    "  test_images_paths = os.listdir('dataset/prediction')\n",
    "  for path in test_images_paths:\n",
    "    image_path = os.path.join('dataset/prediction', path)\n",
    "\n",
    "    image = keras_image.load_img(image_path,\n",
    "                                 color_mode='grayscale',\n",
    "                                 target_size=(width, width))\n",
    "    x = keras_image.img_to_array(image)\n",
    "    x /= 255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    test_image = np.vstack([x])\n",
    "    result = model.predict(test_image, batch_size=8)\n",
    "\n",
    "    print(image_path)\n",
    "    print(classes[np.argmax(result)])\n",
    "\n",
    "    preview = plt_image.imread(image_path)\n",
    "    plt.imshow(preview)\n",
    "    plt.show()\n",
    "  return print('Prediction Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQmCyCco55zr"
   },
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wvIyZ32w6pqr",
    "outputId": "423a0cf2-542d-4ced-eb91-0a4958db2b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1762 images belonging to 20 classes.\n",
      "Found 979 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "SIZE = 100 #Image width\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                   rotation_range=20,\n",
    "                                   zoom_range=0.2,\n",
    "                                   shear_range=0.1)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'dataset/training',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "val_generator = validation_datagen.flow_from_directory(\n",
    "    'dataset/testing',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkIzOgAFfTaL"
   },
   "source": [
    "# Build baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "n2-Jwsa0Beqy"
   },
   "outputs": [],
   "source": [
    "reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto',\n",
    "                    min_delta=0.0001, cooldown=3, min_lr=1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callbacks for Early Stopping\n",
    "\n",
    "We decided to write our own callback class for early stopping for some flexibility. Early stop if accuracy already hit more than 80% and didn't significantly increased after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "eIuUPT9NlfiR"
   },
   "outputs": [],
   "source": [
    "class early_stop(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, threshold=0.8, patience=3, significance_val=0.001):\n",
    "    super(early_stop, self).__init__()\n",
    "\n",
    "    self.threshold = threshold                                                    #Minimum accuracy\n",
    "    self.patience = patience                                                      #Limit epoch to wait if accuracy no longer increased\n",
    "    self.significance_val = significance_val                                      #Significant value to mark improvement in accuracy\n",
    "    self.best_weights = None                                                      #Best weights record\n",
    "    self.is_reach_threshold = False                                               #Track if already reach threshold value or not\n",
    "    self.best_epoch = 0                                                           #Track best epoch that produce best weights\n",
    "\n",
    "  def on_train_begin(self, logs=None):\n",
    "    self.previous_acc = 0                                                         #Track previous accuracy\n",
    "    self.wait = 0                                                                 #Track epoch when accuracy no longer increased\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    self.current_acc = logs.get('val_accuracy')                                   #Current epoch's validation accuracy\n",
    "\n",
    "    if self.is_reach_threshold:\n",
    "      if self.current_acc - self.previous_acc > self.significance_val:\n",
    "        self.wait = 0\n",
    "        self.previous_acc = self.current_acc\n",
    "        self.best_weights = self.model.get_weights()\n",
    "        self.best_epoch = epoch\n",
    "        print('\\n[INFO] Best weights updated!')\n",
    "      else:\n",
    "        self.wait += 1\n",
    "        print(f'\\n[INFO] Current wait = {self.wait}')\n",
    "        if self.wait >= self.patience:\n",
    "          self.model.stop_training = True\n",
    "          print(f'\\n[INFO] Validation Accuracy didnt increased after {self.patience} epochs, training stopped...')\n",
    "          self.model.set_weights(self.best_weights)\n",
    "          print(f'\\n[INFO] Restoring best weights from epoch {self.best_epoch + 1} val_acc')\n",
    "    \n",
    "    if self.current_acc > self.threshold and not self.is_reach_threshold:\n",
    "      self.is_reach_threshold = True\n",
    "      self.previous_acc = self.current_acc\n",
    "      self.best_weights = self.model.get_weights()\n",
    "      self.best_epoch = epoch\n",
    "      print('\\n[INFO] Threshold reached! Best weights start here...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Sequential([Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
    "                         MaxPool2D(2, 2),\n",
    "                         Conv2D(32, (3, 3), activation='relu'),\n",
    "                         MaxPool2D(2, 2),\n",
    "                         Conv2D(32, (3, 3), activation='relu'),\n",
    "                         MaxPool2D(2, 2),\n",
    "                         Conv2D(64, (3, 3), activation='relu'),\n",
    "                         MaxPool2D(2, 2),\n",
    "                         Conv2D(64, (3, 3), activation='relu'),\n",
    "                         MaxPool2D(2, 2),\n",
    "                         Flatten(),\n",
    "                         Dropout(0.5),\n",
    "                         Dense(128, activation='relu'),\n",
    "                         Dense(20, activation='softmax')])\n",
    "\n",
    "base_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS = '/content/gdrive/My Drive/HanacarakaAI/logs/'\n",
    "NAME = f'improved_model-{time.time()}'\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(LOGS, NAME), histogram_freq=1)\n",
    "\n",
    "early_stop = early_stop(patience=20)\n",
    "\n",
    "%tensorboard --logdir='/content/gdrive/My Drive/HanacarakaAI/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_hist = base_model.fit(\n",
    "    train_generator,\n",
    "    epochs = 100,\n",
    "    steps_per_epoch = int(1762/32),\n",
    "    validation_data = val_generator,\n",
    "    validation_steps= int(979/32),\n",
    "    callbacks=[reduce_on_plateau, early_stop, tensorboard],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(base_model_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/content/gdrive/My Drive/HanacarakaAI/model/baseline'\n",
    "tf.saved_model.save(improved_model, os.path.join(MODEL_PATH, 'saved_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(improved_model, os.path.join(MODEL_PATH, 'improved_model.h5'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8VZ+4nag0TNJbOM26J5uM",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Hanacaraka AI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
